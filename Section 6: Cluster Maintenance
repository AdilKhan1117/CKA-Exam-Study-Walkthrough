
OS Upgrades:

Scenerio:
- If you had 1 Master node and 3 worker nodes
- Worker 1 has 3 application on 2 application running on it, blue and green, if this worker node went down, application blue -
  would be fine since its also running on another worker node however cause green is only running on 1 worker node, users will be impacted, 
  what will happen?
- If the worker node,comes up quickly the kubelet will place the application on Worker 1 again.
- However if 5 min have passed the kubelet will terminate these pods. 
- of the pods were part of a replicaset, they are recreated on another worker node.
- The time it waits for a pod to come online, is called a 'Pod eviction timeout' default value of 5 min. 
- When a node goes offline, the master node waits 5 min to consider the worker node being dead. 
- When the node comes back online after the podeviction timeout, it comes back blank with no pods on it. 
- so since the green pod was not part of a replicaset, it would automatically disapear since it past the podeviction period and not part of the replicatset.
- Since we do not know that a node will be back online within 5 min or at all.
    - You can drain the nodes to put the pods on other nodes gracefully. they are simply recreated on another node. 
    - kubectl drain node-1, the node which is being upgraded 
    - Node marked as cordorned which means it is un-scheduleble 
    - To bring it back into being active in the cluster, you can do this command "kubectl uncordon node-1
    - kubectl cordon node-1 (marks the node as cordoned, which means unscxhedulable) 

Practice test: OS Upgrade

1. Let us explore the environment first. How many nodes do you see in the cluster? =. kubectl get nodes = 2
2. How many applications do you see hosted on the cluster? = kubectl get deployments = 1
3. Which nodes are the applications hosted on? = kubectl get pod -o wide = controlplane & node1 
4. We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable. 
   below error:
    kubectl drain node01
    node/node01 cordoned
    error: unable to drain node "node01" due to error:cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-ds-9wmgm, kube-system/kube-proxy-n79c4, continuing command...
    There are pending nodes to be drained:
    node01
    cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/kube-flannel-ds-9wmgm, kube-system/kube-proxy-n79c4
    = kubectl drain node01 --ignore-daemonsets
5. What nodes are the apps on now? = controlplane
6. The maintenance tasks have been completed. Configure the node node01 to be schedulable again = kubectl uncordon node01
7. How many pods are scheduled on node01 now? = 0
8. Why are there no pods on node01? only when new pods are created, they will be scheduled
9. Why are the pods placed on the controlplane node? controlplane node does not have any taint
10. ////
11. We need to carry out a maintenance activity on node01 again. Try draining the node again using the same command as before: 
    kubectl drain node01 --ignore-daemonsets 
12. Why did the drain command fail on node01? It worked the first time! = There is 1 pod in there, which isnt part of replicaset
13. What is the name of the POD hosted on node01 that is not part of a replicaset? = 
14. What would happen to hr-app if node01 is drained forcefully? A forceful drain of the node will delete any pod that is not part of a replicaset.
15. /////
16. hr-app is a critical app and we do not want it to be removed and we do not want to schedule any more pods on node01.
    Mark node01 as unschedulable so that no new pods are scheduled on this node.
    = kubectl cordon node01

Make sure that hr-app is not affected.

kubernetes version:

example:
v1.11.3
1 = Major version
11 = minor version
3 = patch

Cluster Process:

- Each kubernetes component have their version of software
- kube-api-server
  controller-manager 
  kube-scheduler
  kubelet
  kube-proxy
  kubectl
  etcd cluster
  CoreDNS
  
- Each componenet can run on different versions not mandotory
- kube-api server is the main component where all other components talk to, they cannot be higher than the Kube-API Server
- However the kube-controller can be a version higher than kube api-server or even same leve. 
- recommended approach when upgrading is to upgrade 1 minor version at a time 
- 
